\section{Conclusion and Future Work}
\note{From C\&B: In contrast with these previous works, the pre-sented research herein offers a sequential Bayesian formulation that addresses joint dependence of probabilities, the presence of both false-negative and false-positive detections, and constraints on searcher motion}

\note{however,  such  costly near-optimal approaches may be less desirable than algorithms that provide “good enough” solutions, where the application re-quires computationally limited platforms, such as educational robots  or  miniaturized  embedded  system}

This chapter described a system which was designed to solve the problem of \textit{target localisation} using RAVs. Our system was designed on the basis of previous work, outlined in section <>. The system's performance was analysed using Monte Carlo simulation and we explored the trade-off between minimising time-to-decision (TTD) with drawing incorrect conclusions. The results largely matched what we expected to see, but they offer a solid basis on which to set system parameters in order to achieve a desired level of performance. We evaluated four search strategies. Two were adaptive, $\epsilon$-greedy search and Saccadic search, meaning they sampled the search region in a biased manner in the hope of converging quickly on the target location. Two were unbiased, Sweep search and Random seach, which offered a baseline for comparison and facilitated analysis which was useful in showing how the system reacts to perturbations in the values of the parameters.

We first found that the shape of the initial belief distribution has a large bearing on the time-to-decision, where the use of an uninformed initial belief distribution causes a large increase in the search time relative to the use of an initial belief distribution that is aligned with the shape of the true distribution. The effect was greatest while using the adaptive search methods, which offered respective savings of 80.1\% and 85.3\% for the $\epsilon$-greedy search and Saccadic search strategies. The results also showed that the rate at which the system incorrectly concluded that the target was not present also dropped significantly, with respective reductions of 80.1\% and 75.6.3\% for the $\epsilon$-greedy search and Saccadic search strategies. \par

When the initial cumulative belief that the target was present for a uniform distribution was set to 0.25, we observed a drop in the mean time decision for all search strategies. The adaptive strategies followed up on false positives, which drove the cumulative below the lower SPRT threshold prematurely. the non-adaptive strategies also experienced enough negative observations in relation to positives to terminate prematurely. This led to an inflated rate at which the system concluded the source was not present. In the case where the initial cumulative belief that the target was present started at 0.75, the adaptive methods saw a slight increase in the mean TTD relative to the case where the initial cumulative belief was set to 0.5. The evidence suggested that since a comparable number of consecutive positive observations at the true target location would push the agent's belief over the SPRT upper threshold, the relatively high number of samples needed to make a negative conclusion was the cause. The non-adaptive methods saw a decrease in the mean TTD because the initial belief started closer to the boundary and required  fewer positive observations to cross the upper SPRT threshold.\par


When we set the sensor model parameters to under-estimate the actual false positive and false negative rates of observations, the results we observed matched our expectations. Since the model was more confident about observations, the agent's belief fluctuated more aggressively, which made it easier to cross the upper and lower decision thresholds. This led to a drop in the mean TTD for all search strategies. Conversely, when the sensor model parameters over-estimated the fpr and fnr of observations, the agent's belief fluctuated in a much more gentle manner, which meant it took far more observations to reach the threshold set by the SPRT. When the sensor model parameters were under-estimated, the proportion of simulation runs where the target was incorrectly localised shot up, since the relative true positive rate was lower than in the case where the sensor model fpr and fnr were set to 0.2 and 0.15 respectively.



\subsection{Future Work}
Future work for the problem of target localisation has been outlined in <list previous works>. We identify the following as problems that could be tackled in future research:

\begin{enumerate}
    \item In our simplified approach, we allowed agents to move to any location in the grid representation of their environment on each timestep. In reality, this would cause the actual search time to be very large as the agents could traverse long distances back and forth across the search grid on each time step. Taking this into account would yield a much more practical system.
    \item Similar to point 1., we did not take into account battery limitations for the RAV agents. In reality, they would need to periodically recharge while carrying out the search, as mentioned in section \ref{sec:SceneSurveyingBatteryConstraints} of chapter \ref{chapter:SceneSurveying}. We hypothesise that a battery model could be learned using the Expectation-Maximisation (E-M) algorithm \cite{Dempster1977MaximumAlgorithm}, discussed in section \ref{subsubsec:EMAlgo}. This could then be integrated with the search strategy to prevent the RAV from running out of power.
    \item In this work, all simulations were ran with a target present. Future work could investigate system performance with the absence of a target, or with the presence of false targets that could be considered as an adversarial measure taken to fool the system.
    \item We assumed that agents could localise \textit{themselves} accurately, which is not always the case. Research into how agents can perform the search without a deterministic location sensor <link to other research>.
    \item Our simulation involving multiple agents assumed that they are homogeneous. Future work could explore the use of heterogeneous agents in carrying out the target localisation task. This could involve using agents with different sensing capabilities, different operational speeds, different battery lives and different search strategies. Future work could also investigate how to optimise various configurations to provide a more robust and efficent system.
    \item The performance measure discussed in section \ref{sssection:PerfMeas} only takes into account the time taken to localise the target. It could be modified to take into account other factors such as energy expended by the agents. This may allow for a more realistic trade-off than solely focusing on making the correct decision in the least possible amount of time.
    \item It is becoming more common for RAVs to integrate a multitude of sensors in their platforms. For example, in the ROCSAFE project \cite{rocsafeNUIG}, advanced lightweight senors are being developed to be used with a RAV to remotely detect potentially hazardous substances. Multi-sensor fusion techniques have been shown in certain cases to be highly effective in identifying outliers and creating more robust estimators of true environment state. The review paper \cite{Khaleghi2013MultisensorState-of-the-art} discusses challenges and promising future areas related to research involving the fusion of data from multiple sensors. Exploring the improvements that a multi-sensor approach could potentially provide could be a future avenue of research.
    \item  We followed the approach of \cite{Chung2007ASearch} in developing a sensor model. The approach is very general, but is highly susceptible to providing poor results due to miscalibration, evident in table \ref{table:MiscalibratedSensor}. Developing a more advanced sensor model that is specific to common tasks, such as object detection, may yield a more robust and efficient system. For example, \cite{Symington2010ProbabilisticUAVs} outlines a sensor model which was calibrated with varying heights of the RAV and \cite{ThrunLearningModels} outlines sensor models commonly used with robotic vehicles.
    \item Running the system in a realistic setting would offer insight in the practicality of the system. For example, we ran the agent software on a desktop machine, but in reality it would likely be running on an embedded system with a reduced computational ability relative to a desktop. This may show the system is infeasible due to constraints such as memory size or processor speed. Future research could involve running the system on realistic hardware to evaluate its feasibility. In the future, we intend to run the system using the high-fidelity simulation discussed in chapter \ref{chap:HighFidelitySim}, with the agent software running on raspberry pis. This will give a stronger indication of how the system may potentially perform in the real world.
    \item Further theoretical analysis of the system could offer insight into how it could be improved. In \cite{Chung2012AnalysisStrategies} and \cite{Chung2009ProbabilisticAgents}, special cases area analysed in order to elicit properties, such as the rate of change of agent belief, which can be useful in devising strategies to improve the search performance.
\end{enumerate}