\subsection{Estimated State}
\workinprogress
Since the agent operates in a partially observable environment, the agent's internal state representation of the environment needs to take into account the fact that the current environment state is not certain. For this reason, a probability distribution over possible environment states is used by the agent to internally represent the environment state. The internal agent state is described mathematically by 
\[p(x_t | e_{1:t}, u_{1:t})\]
which is the distribution of possible world states given all evidence and control actions up to the current time step, where $x_t$, $e_t$ and $u_t$ follow the conventions of being the hidden state variables, the evidence variables and the control action variables respectively. The agent updates this state using the world model specified in the preceding chapter, using the filtering algorithm explained in Section \ref{subsec:BGInfAlgos}. There are a couple of noteworthy points:
\begin{itemize}
    \item A distribution that has a single sharp peak would indicate that the agent believes that the target is at a specific location with high confidence. This is clearly preferred to a flat, uniform distribution representing uncertainty in relation to where the target might be.
    \item The only true source of uncertainty in the world state is introduced by the sensor. Therefore, analysis of this state representation in relation to the sensor model should provide insight into how the system performs.
\end{itemize}
\par In order to update the estimated state of the agent, the below equations are used, which are described in their general form in Section \ref{subsec:BGInfAlgos}. For brevity $x_t$ denotes the hidden state variables, $e_t$ denotes the evidence variables and $u_t$ denotes the control action taken by the agent. The equations only describe the estimated state update for move actions, since if the agent terminates the search a terminal state is deterministically entered. The $SearchStatus$ variable is omitted from the equations since it only effects the equations if the $Action$ variable is to terminate the search.
%\footnotesize
%\begin{equation}
\scriptsize
%\begin{equation}\label{eqn:SearchStatus}
%\[
    %\begin{cases}\label{eqn:AgentUpdateEquation}
    \begin{center}
    \begin{align}
    \begin{split}
    %\centering
        p(AgentLoc_t = x, TargetLoc_t = y | e_{1:t}, u_{1:t}) = &\\
       \begin{cases}
            \eta \alpha p(AgentLoc_{t-1} = x, TargetLoc_t = y | e_{1:t-1}, u_{1:t-1}) \text{ if $e_t$ is a positive reading and $AgentLoc_t \neq TargetLoc_t$} \\
            \eta \beta p(X_{t-1}=x_t | e_{1:t-1}, u_{1:t-1}) \text{ if $e_t$ is a negative reading and $Agent_loc_t$ = $TargetLoc_t$} \\
            \eta (1-\alpha) p(X_{t-1}=x_t | e_{1:t-1}, u_{1:t-1}) \text{ if $e_t$ is a negative reading and $Agent_loc_t \neq TargetLoc_t$} \\
            \eta (1-\beta) p(X_{t-1}=x_t | e_{1:t-1}, u_{1:t-1}) \text{ if $e_t$ is a positive reading and $Agent_loc_t$ = $TargetLoc_t$}
        %\end{cases}
%\eta O_{t} T^{T} f_{1:t-1} 
    \end{cases}
    \end{split}
    \end{align}
    \end{center}
%\]
%\end{equation}
\normalsize

$\eta$ is a normalising factor, $\alpha$ is the probability of a sensor false positive detection and $\beta$ is the probability of a sensor false negative detection.